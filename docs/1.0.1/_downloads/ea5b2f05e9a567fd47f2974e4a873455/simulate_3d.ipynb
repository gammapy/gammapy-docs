{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 3D map simulation\n\nSimulate a 3D observation of a source with the CTA 1DC response and fit it with the assumed source model.\n\n## Prerequisites\n\n-  Knowledge of 3D extraction and datasets used in gammapy, see for\n   example the :doc:`/tutorials/starting/analysis_1` tutorial.\n\n## Context\n\nTo simulate a specific observation, it is not always necessary to\nsimulate the full photon list. For many uses cases, simulating directly\na reduced binned dataset is enough: the IRFs reduced in the correct\ngeometry are combined with a source model to predict an actual number of\ncounts per bin. The latter is then used to simulate a reduced dataset\nusing Poisson probability distribution.\n\nThis can be done to check the feasibility of a measurement (performance\n/ sensitivity study), to test whether fitted parameters really provide a\ngood fit to the data etc.\n\nHere we will see how to perform a 3D simulation of a CTA observation,\nassuming both the spectral and spatial morphology of an observed source.\n\n**Objective: simulate a 3D observation of a source with CTA using the\nCTA 1DC response and fit it with the assumed source model.**\n\n## Proposed approach\n\nHere we can\u2019t use the regular observation objects that are connected to\na `DataStore`. Instead we will create a fake\n`~gammapy.data.Observation` that contain some pointing information and\nthe CTA 1DC IRFs (that are loaded with `~gammapy.irf.load_cta_irfs`).\n\nThen we will create a `~gammapy.datasets.MapDataset` geometry and\ncreate it with the `~gammapy.makers.MapDatasetMaker`.\n\nThen we will be able to define a model consisting of a\n`~gammapy.modeling.models.PowerLawSpectralModel` and a\n`~gammapy.modeling.models.GaussianSpatialModel`. We will assign it to\nthe dataset and fake the count data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and versions\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nimport matplotlib.pyplot as plt\n\n# %matplotlib inline\nfrom IPython.display import display\nfrom gammapy.data import Observation, observatory_locations\nfrom gammapy.datasets import MapDataset\nfrom gammapy.irf import load_cta_irfs\nfrom gammapy.makers import MapDatasetMaker, SafeMaskMaker\nfrom gammapy.maps import MapAxis, WcsGeom\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    FoVBackgroundModel,\n    GaussianSpatialModel,\n    Models,\n    PowerLawSpectralModel,\n    SkyModel,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will simulate using the CTA-1DC IRFs shipped with gammapy\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Loading IRFs\nirfs = load_cta_irfs(\n    \"$GAMMAPY_DATA/cta-1dc/caldb/data/cta/1dc/bcf/South_z20_50h/irf_file.fits\"\n)\n\n# Define the observation parameters (typically the observation duration and the pointing position):\nlivetime = 2.0 * u.hr\npointing = SkyCoord(0, 0, unit=\"deg\", frame=\"galactic\")\n\n# Define map geometry for binned simulation\nenergy_reco = MapAxis.from_edges(\n    np.logspace(-1.0, 1.0, 10), unit=\"TeV\", name=\"energy\", interp=\"log\"\n)\ngeom = WcsGeom.create(\n    skydir=(0, 0),\n    binsz=0.02,\n    width=(6, 6),\n    frame=\"galactic\",\n    axes=[energy_reco],\n)\n# It is usually useful to have a separate binning for the true energy axis\nenergy_true = MapAxis.from_edges(\n    np.logspace(-1.5, 1.5, 30), unit=\"TeV\", name=\"energy_true\", interp=\"log\"\n)\n\nempty = MapDataset.create(geom, name=\"dataset-simu\", energy_axis_true=energy_true)\n\n# Define sky model to used simulate the data.\n# Here we use a Gaussian spatial model and a Power Law spectral model.\nspatial_model = GaussianSpatialModel(\n    lon_0=\"0.2 deg\", lat_0=\"0.1 deg\", sigma=\"0.3 deg\", frame=\"galactic\"\n)\nspectral_model = PowerLawSpectralModel(\n    index=3, amplitude=\"1e-11 cm-2 s-1 TeV-1\", reference=\"1 TeV\"\n)\nmodel_simu = SkyModel(\n    spatial_model=spatial_model,\n    spectral_model=spectral_model,\n    name=\"model-simu\",\n)\n\nbkg_model = FoVBackgroundModel(dataset_name=\"dataset-simu\")\n\nmodels = Models([model_simu, bkg_model])\nprint(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, comes the main part of dataset simulation. We create an in-memory\nobservation and an empty dataset. We then predict the number of counts\nfor the given model, and Poission fluctuate it using `fake()` to make\na simulated counts maps. Keep in mind that it is important to specify\nthe `selection` of the maps that you want to produce\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create an in-memory observation\nlocation = observatory_locations[\"cta_south\"]\nobs = Observation.create(\n    pointing=pointing, livetime=livetime, irfs=irfs, location=location\n)\nprint(obs)\n\n# Make the MapDataset\nmaker = MapDatasetMaker(selection=[\"exposure\", \"background\", \"psf\", \"edisp\"])\n\nmaker_safe_mask = SafeMaskMaker(methods=[\"offset-max\"], offset_max=4.0 * u.deg)\n\ndataset = maker.run(empty, obs)\ndataset = maker_safe_mask.run(dataset, obs)\nprint(dataset)\n\n# Add the model on the dataset and Poission fluctuate\ndataset.models = models\ndataset.fake()\n# Do a print on the dataset - there is now a counts maps\nprint(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now use this dataset as you would in all standard analysis. You can plot\nthe maps, or proceed with your custom analysis. In the next section, we\nshow the standard 3D fitting as in :doc:`/tutorials/analysis-3d/analysis_3d`\ntutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# To plot, eg, counts:\nplt.figure()\ndataset.counts.smooth(0.05 * u.deg).plot_interactive(add_cbar=True, stretch=\"linear\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit\n\nIn this section, we do a usual 3D fit with the same model used to\nsimulated the data and see the stability of the simulations. Often, it\nis useful to simulate many such datasets and look at the distribution of\nthe reconstructed parameters.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models_fit = models.copy()\n\n# We do not want to fit the background in this case, so we will freeze the parameters\nmodels_fit[\"dataset-simu-bkg\"].spectral_model.norm.frozen = True\nmodels_fit[\"dataset-simu-bkg\"].spectral_model.tilt.frozen = True\n\ndataset.models = models_fit\nprint(dataset.models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit = Fit(optimize_opts={\"print_level\": 1})\nresult = fit.run(datasets=[dataset])\nplt.figure()\ndataset.plot_residuals_spatial(method=\"diff/sqrt(model)\", vmin=-0.5, vmax=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the injected and fitted models:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\n    \"True model: \\n\",\n    model_simu,\n    \"\\n\\n Fitted model: \\n\",\n    models_fit[\"model-simu\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the errors on the fitted parameters from the parameter table\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(result.parameters.to_table())\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}