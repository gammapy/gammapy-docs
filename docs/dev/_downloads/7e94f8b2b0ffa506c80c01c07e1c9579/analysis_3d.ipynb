{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# 3D detailed analysis\n\nPerform detailed 3D stacked and joint analysis.\n\nThis tutorial does a 3D map based analsis on the galactic center, using\nsimulated observations from the CTA-1DC. We will use the high level\ninterface for the data reduction, and then do a detailed modelling. This\nwill be done in two different ways:\n\n-  stacking all the maps together and fitting the stacked maps\n-  handling all the observations separately and doing a joint fitting on\n   all the maps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\nimport numpy as np\nfrom scipy.stats import norm\nimport astropy.units as u\nfrom regions import CircleSkyRegion\n\n# %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom gammapy.analysis import Analysis, AnalysisConfig\nfrom gammapy.datasets import MapDataset\nfrom gammapy.estimators import ExcessMapEstimator\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLawSpectralModel,\n    FoVBackgroundModel,\n    Models,\n    PointSpatialModel,\n    SkyModel,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gammapy.utils.check import check_tutorials_setup\n\ncheck_tutorials_setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis configuration\n\nIn this section we select observations and define the analysis\ngeometries, irrespective of joint/stacked analysis. For configuration of\nthe analysis, we will programmatically build a config file from scratch.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config = AnalysisConfig()\n# The config file is now empty, with only a few defaults specified.\nprint(config)\n\n# Selecting the observations\nconfig.observations.datastore = \"$GAMMAPY_DATA/cta-1dc/index/gps/\"\nconfig.observations.obs_ids = [110380, 111140, 111159]\n\n# Defining a reference geometry for the reduced datasets\n\nconfig.datasets.type = \"3d\"  # Analysis type is 3D\n\nconfig.datasets.geom.wcs.skydir = {\n    \"lon\": \"0 deg\",\n    \"lat\": \"0 deg\",\n    \"frame\": \"galactic\",\n}  # The WCS geometry - centered on the galactic center\nconfig.datasets.geom.wcs.width = {\"width\": \"10 deg\", \"height\": \"8 deg\"}\nconfig.datasets.geom.wcs.binsize = \"0.02 deg\"\n\n# Cutout size (for the run-wise event selection)\nconfig.datasets.geom.selection.offset_max = 3.5 * u.deg\nconfig.datasets.safe_mask.methods = [\"aeff-default\", \"offset-max\"]\n\n# We now fix the energy axis for the counts map - (the reconstructed energy binning)\nconfig.datasets.geom.axes.energy.min = \"0.1 TeV\"\nconfig.datasets.geom.axes.energy.max = \"10 TeV\"\nconfig.datasets.geom.axes.energy.nbins = 10\n\n# We now fix the energy axis for the IRF maps (exposure, etc) - (the true energy binning)\nconfig.datasets.geom.axes.energy_true.min = \"0.08 TeV\"\nconfig.datasets.geom.axes.energy_true.max = \"12 TeV\"\nconfig.datasets.geom.axes.energy_true.nbins = 14\n\nprint(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration for stacked and joint analysis\n\nThis is done just by specfiying the flag on `config.datasets.stack`.\nSince the internal machinery will work differently for the two cases, we\nwill write it as two config files and save it to disc in YAML format for\nfuture reference.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "config_stack = config.copy(deep=True)\nconfig_stack.datasets.stack = True\n\nconfig_joint = config.copy(deep=True)\nconfig_joint.datasets.stack = False\n\n# To prevent unnecessary cluttering, we write it in a separate folder.\npath = Path(\"analysis_3d\")\npath.mkdir(exist_ok=True)\nconfig_joint.write(path=path / \"config_joint.yaml\", overwrite=True)\nconfig_stack.write(path=path / \"config_stack.yaml\", overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacked analysis\n\n### Data reduction\n\nWe first show the steps for the stacked analysis and then repeat the\nsame for the joint analysis later\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Reading yaml file:\nconfig_stacked = AnalysisConfig.read(path=path / \"config_stack.yaml\")\n\nanalysis_stacked = Analysis(config_stacked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "select observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analysis_stacked.get_observations()\n\n# run data reduction\nanalysis_stacked.get_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have one final dataset, which you can print and explore\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_stacked = analysis_stacked.datasets[\"stacked\"]\nprint(dataset_stacked)\n\n# To plot a smooth counts map\ndataset_stacked.counts.smooth(0.02 * u.deg).plot_interactive(add_cbar=True)\n\n# And the background map\ndataset_stacked.background.plot_interactive(add_cbar=True)\n\n# We can quickly check the PSF\ndataset_stacked.psf.peek()\n\n# And the energy dispersion in the center of the map\ndataset_stacked.edisp.peek()\n\n# You can also get an excess image with a few lines of code:\nexcess = dataset_stacked.excess.sum_over_axes()\nexcess.smooth(\"0.06 deg\").plot(stretch=\"sqrt\", add_cbar=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modeling and fitting\n\nNow comes the interesting part of the analysis - choosing appropriate\nmodels for our source and fitting them.\n\nWe choose a point source model with an exponential cutoff power-law\nspectrum.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To perform the fit on a restricted energy range, we can create a\nspecific *mask*. On the dataset, the `mask_fit` is a `Map` sharing\nthe same geometry as the `MapDataset` and containing boolean data.\n\nTo create a mask to limit the fit within a restricted energy range, one\ncan rely on the `~gammapy.maps.Geom.energy_mask()` method.\n\nFor more details on masks and the techniques to create them in gammapy,\nplease checkou the dedicated :doc:`/tutorials/api/mask_maps` tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_stacked.mask_fit = dataset_stacked.counts.geom.energy_mask(\n    energy_min=0.3 * u.TeV, energy_max=None\n)\n\nspatial_model = PointSpatialModel(\n    lon_0=\"-0.05 deg\", lat_0=\"-0.05 deg\", frame=\"galactic\"\n)\nspectral_model = ExpCutoffPowerLawSpectralModel(\n    index=2.3,\n    amplitude=2.8e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n    reference=1.0 * u.TeV,\n    lambda_=0.02 / u.TeV,\n)\n\nmodel = SkyModel(\n    spatial_model=spatial_model,\n    spectral_model=spectral_model,\n    name=\"gc-source\",\n)\n\nbkg_model = FoVBackgroundModel(dataset_name=\"stacked\")\nbkg_model.spectral_model.norm.value = 1.3\n\nmodels_stacked = Models([model, bkg_model])\n\ndataset_stacked.models = models_stacked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit = Fit(optimize_opts={\"print_level\": 1})\nresult = fit.run(datasets=[dataset_stacked])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit quality assessment and model residuals for a `MapDataset`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the results dictionary to see if the fit converged:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check best-fit parameters and error estimates:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models_stacked.to_parameters_table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A quick way to inspect the model residuals is using the function\n`~MapDataset.plot_residuals_spatial()`. This function computes and\nplots a residual image (by default, the smoothing radius is `0.1 deg`\nand `method=diff`, which corresponds to a simple `data - model`\nplot):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_stacked.plot_residuals_spatial(method=\"diff/sqrt(model)\", vmin=-1, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The more general function `~MapDataset.plot_residuals()` can also\nextract and display spectral residuals in a region:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "region = CircleSkyRegion(spatial_model.position, radius=0.15 * u.deg)\n\ndataset_stacked.plot_residuals(\n    kwargs_spatial=dict(method=\"diff/sqrt(model)\", vmin=-1, vmax=1),\n    kwargs_spectral=dict(region=region),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This way of accessing residuals is quick and handy, but comes with\nlimitations. For example: - In case a fitting energy range was defined\nusing a `MapDataset.mask_fit`, it won\u2019t be taken into account.\nResiduals will be summed up over the whole reconstructed energy range -\nIn order to make a proper statistic treatment, instead of simple\nresiduals a proper residuals significance map should be computed\n\nA more accurate way to inspect spatial residuals is the following:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = ExcessMapEstimator(\n    correlation_radius=\"0.1 deg\",\n    selection_optional=[],\n    energy_edges=[0.1, 1, 10] * u.TeV,\n)\n\nresult = estimator.run(dataset_stacked)\n\nresult[\"sqrt_ts\"].plot_grid(\n    figsize=(12, 4), cmap=\"coolwarm\", add_cbar=True, vmin=-5, vmax=5, ncols=2\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Distribution of residuals significance in the full map geometry:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# TODO: clean this up\nsignificance_data = result[\"sqrt_ts\"].data\n\n# #Remove bins that are inside an exclusion region, that would create an artificial peak at significance=0.\n# #For now these lines are useless, because to_image() drops the mask fit\n# mask_data = dataset_image.mask_fit.sum_over_axes().data\n# excluded = mask_data == 0\n# significance_data = significance_data[~excluded]\nselection = np.isfinite(significance_data) & ~(significance_data == 0)\nsignificance_data = significance_data[selection]\n\nplt.hist(significance_data, density=True, alpha=0.9, color=\"red\", bins=40)\nmu, std = norm.fit(significance_data)\n\nx = np.linspace(-5, 5, 100)\np = norm.pdf(x, mu, std)\n\nplt.plot(\n    x,\n    p,\n    lw=2,\n    color=\"black\",\n    label=r\"$\\mu$ = {:.2f}, $\\sigma$ = {:.2f}\".format(mu, std),\n)\nplt.legend(fontsize=17)\nplt.xlim(-5, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joint analysis\n\nIn this section, we perform a joint analysis of the same data. Of\ncourse, joint fitting is considerably heavier than stacked one, and\nshould always be handled with care. For brevity, we only show the\nanalysis for a point source fitting without re-adding a diffuse\ncomponent again.\n\n### Data reduction\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Read the yaml file from disk\nconfig_joint = AnalysisConfig.read(path=path / \"config_joint.yaml\")\nanalysis_joint = Analysis(config_joint)\n\n# select observations:\nanalysis_joint.get_observations()\n\n# run data reduction\nanalysis_joint.get_datasets()\n\n# You can see there are 3 datasets now\nprint(analysis_joint.datasets)\n\n# You can access each one by name or by index, eg:\nprint(analysis_joint.datasets[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the data reduction stage, it is nice to get a quick summary info\non the datasets. Here, we look at the statistics in the center of Map,\nby passing an appropriate `region`. To get info on the entire spatial\nmap, omit the region argument.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analysis_joint.datasets.info_table()\n\nmodels_joint = Models()\n\nmodel_joint = model.copy(name=\"source-joint\")\nmodels_joint.append(model_joint)\n\nfor dataset in analysis_joint.datasets:\n    bkg_model = FoVBackgroundModel(dataset_name=dataset.name)\n    models_joint.append(bkg_model)\n\nprint(models_joint)\n\n# and set the new model\nanalysis_joint.datasets.models = models_joint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit_joint = Fit()\nresult_joint = fit_joint.run(datasets=analysis_joint.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit quality assessment and model residuals for a joint `Datasets`\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the results dictionary to see if the fit converged:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_joint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check best-fit parameters and error estimates:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(models_joint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the joint dataset is made of multiple datasets, we can either: -\nLook at the residuals for each dataset separately. In this case, we can\ndirectly refer to the section\n`Fit quality and model residuals for a MapDataset` in this notebook -\nLook at a stacked residual map.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the latter case, we need to properly stack the joint dataset before\ncomputing the residuals:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# TODO: clean this up\n\n# We need to stack on the full geometry, so we use to geom from the stacked counts map.\nstacked = MapDataset.from_geoms(**dataset_stacked.geoms)\n\nfor dataset in analysis_joint.datasets:\n    # TODO: Apply mask_fit before stacking\n    stacked.stack(dataset)\n\nstacked.models = [model_joint]\n\nstacked.plot_residuals_spatial(vmin=-1, vmax=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can access the stacked model residuals as previously shown in\nthe section `Fit quality and model residuals for a MapDataset` in this\nnotebook.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, let us compare the spectral results from the stacked and joint\nfit:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_spectrum(model, result, label, color):\n    spec = model.spectral_model\n    energy_bounds = [0.3, 10] * u.TeV\n    spec.plot(energy_bounds=energy_bounds, energy_power=2, label=label, color=color)\n    spec.plot_error(energy_bounds=energy_bounds, energy_power=2, color=color)\n\n\nplot_spectrum(model, result, label=\"stacked\", color=\"tab:blue\")\nplot_spectrum(model_joint, result_joint, label=\"joint\", color=\"tab:orange\")\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nNote that this notebook aims to show you the procedure of a 3D analysis\nusing just a few observations. Results get much better for a more\ncomplete analysis considering the GPS dataset from the CTA First Data\nChallenge (DC-1) and also the CTA model for the Galactic diffuse\nemission, as shown in the next image:\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"file://../../_static/DC1_3d.png\">\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\n-  Analyse the second source in the field of view: G0.9+0.1 and add it\n   to the combined model.\n-  Perform modeling in more details - Add diffuse component, get flux\n   points.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}