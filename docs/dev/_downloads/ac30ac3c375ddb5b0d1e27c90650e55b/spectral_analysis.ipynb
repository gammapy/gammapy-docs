{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Spectral analysis\n\nPerform a full region based on-off spectral analysis and fit the resulting datasets.\n\n## Prerequisites\n\n-  Understanding how spectral extraction is performed in Cherenkov\n   astronomy, in particular regarding OFF background measurements.\n-  Understanding the basics data reduction and modeling/fitting process\n   with the gammapy library API as shown in the :doc:`/tutorials/starting/analysis_2`\n   tutorial.\n\n## Context\n\nWhile 3D analyses allow in principle to consider complex field of views\ncontaining overlapping gamma-ray sources, in many cases we might have an\nobservation with a single, strong, point-like source in the field of\nview. A spectral analysis, in that case, might consider all the events\ninside a source (or ON) region and bin them in energy only, obtaining 1D\ndatasets.\n\nIn classical Cherenkov astronomy, the background estimation technique\nassociated with this method measures the number of events in OFF regions\ntaken in regions of the field-of-view devoid of gamma-ray emitters,\nwhere the background rate is assumed to be equal to the one in the ON\nregion.\n\nThis allows to use a specific fit statistics for ON-OFF measurements,\nthe wstat (see `~gammapy.stats.wstat`), where no background model is\nassumed. Background is treated as a set of nuisance parameters. This\nremoves some systematic effects connected to the choice or the quality\nof the background model. But this comes at the expense of larger\nstatistical uncertainties on the fitted model parameters.\n\n**Objective: perform a full region based spectral analysis of 4 Crab\nobservations of H.E.S.S. data release 1 and fit the resulting\ndatasets.**\n\n## Introduction\n\nHere, as usual, we use the `~gammapy.data.DataStore` to retrieve a\nlist of selected observations (`~gammapy.data.Observations`). Then, we\ndefine the ON region containing the source and the geometry of the\n`~gammapy.datasets.SpectrumDataset` object we want to produce. We then\ncreate the corresponding dataset Maker.\n\nWe have to define the Maker object that will extract the OFF counts from\nreflected regions in the field-of-view. To ensure we use data in an\nenergy range where the quality of the IRFs is good enough we also create\na safe range Maker.\n\nWe can then proceed with data reduction with a loop over all selected\nobservations to produce datasets in the relevant geometry.\n\nWe can then explore the resulting datasets and look at the cumulative\nsignal and significance of our source. We finally proceed with model\nfitting.\n\nIn practice, we have to:\n\n- Create a `~gammapy.data.DataStore` pointing to the relevant data\n- Apply an observation selection to produce a list of observations,\n  a `~gammapy.data.Observations` object.\n- Define a geometry of the spectrum we want to produce:\n\n  - Create a `~regions.CircleSkyRegion` for the ON extraction region\n  - Create a `~gammapy.maps.MapAxis` for the energy binnings: one for the\n    reconstructed (i.e.measured) energy, the other for the true energy\n    (i.e.the one used by IRFs and models)\n\n- Create the necessary makers :\n\n  - the spectrum dataset maker : `~gammapy.makers.SpectrumDatasetMaker` -\n    the OFF background maker, here a `~gammapy.makers.ReflectedRegionsBackgroundMaker`\n  - and the safe range maker : `~gammapy.makers.SafeMaskMaker`\n\n- Perform the data reduction loop. And for every observation:\n\n  - Apply the makers sequentially to produce a `~gammapy.datasets.SpectrumDatasetOnOff`\n  - Append it to list of datasets\n\n- Define the `~gammapy.modeling.models.SkyModel` to apply to the dataset.\n- Create a `~gammapy.modeling.Fit` object and run it to fit the model parameters\n- Apply a `~gammapy.estimators.FluxPointsEstimator` to compute flux points for\n  the spectral part of the fit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\n# Check package versions\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom regions import CircleSkyRegion\n\n# %matplotlib inline\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nAs usual, we\u2019ll start with some setup \u2026\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import (\n    Datasets,\n    FluxPointsDataset,\n    SpectrumDataset,\n    SpectrumDatasetOnOff,\n)\nfrom gammapy.estimators import FluxPointsEstimator\nfrom gammapy.estimators.utils import resample_energy_edges\nfrom gammapy.makers import (\n    ReflectedRegionsBackgroundMaker,\n    SafeMaskMaker,\n    SpectrumDatasetMaker,\n)\nfrom gammapy.maps import MapAxis, RegionGeom, WcsGeom\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import (\n    ExpCutoffPowerLawSpectralModel,\n    SkyModel,\n    create_crab_spectral_model,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gammapy.utils.check import check_tutorials_setup\nfrom gammapy.visualization import plot_spectrum_datasets_off_regions\n\ncheck_tutorials_setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data\n\nFirst, we select and load some H.E.S.S. observations of the Crab nebula.\n\nWe will access the events, effective area, energy dispersion, livetime\nand PSF for containment correction.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datastore = DataStore.from_dir(\"$GAMMAPY_DATA/hess-dl3-dr1/\")\nobs_ids = [23523, 23526, 23559, 23592]\nobservations = datastore.get_observations(obs_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Target Region\n\nThe next step is to define a signal extraction region, also known as on\nregion. In the simplest case this is just a\n[CircleSkyRegion](http://astropy-regions.readthedocs.io/en/latest/api/regions.CircleSkyRegion.html)_.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_position = SkyCoord(ra=83.63, dec=22.01, unit=\"deg\", frame=\"icrs\")\non_region_radius = Angle(\"0.11 deg\")\non_region = CircleSkyRegion(center=target_position, radius=on_region_radius)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create exclusion mask\n\nWe will use the reflected regions method to place off regions to\nestimate the background level in the on region. To make sure the off\nregions don\u2019t contain gamma-ray emission, we create an exclusion mask.\n\nUsing http://gamma-sky.net/ we find that there\u2019s only one known\ngamma-ray source near the Crab nebula: the AGN called [RGB\nJ0521+212](http://gamma-sky.net/#/cat/tev/23)_ at GLON = 183.604 deg\nand GLAT = -8.708 deg.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exclusion_region = CircleSkyRegion(\n    center=SkyCoord(183.604, -8.708, unit=\"deg\", frame=\"galactic\"),\n    radius=0.5 * u.deg,\n)\n\nskydir = target_position.galactic\ngeom = WcsGeom.create(\n    npix=(150, 150), binsz=0.05, skydir=skydir, proj=\"TAN\", frame=\"icrs\"\n)\n\nexclusion_mask = ~geom.region_mask([exclusion_region])\nexclusion_mask.plot()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run data reduction chain\n\nWe begin with the configuration of the maker classes:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "energy_axis = MapAxis.from_energy_bounds(\n    0.1, 40, nbin=10, per_decade=True, unit=\"TeV\", name=\"energy\"\n)\nenergy_axis_true = MapAxis.from_energy_bounds(\n    0.05, 100, nbin=20, per_decade=True, unit=\"TeV\", name=\"energy_true\"\n)\n\ngeom = RegionGeom.create(region=on_region, axes=[energy_axis])\ndataset_empty = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true)\n\ndataset_maker = SpectrumDatasetMaker(\n    containment_correction=True, selection=[\"counts\", \"exposure\", \"edisp\"]\n)\nbkg_maker = ReflectedRegionsBackgroundMaker(exclusion_mask=exclusion_mask)\nsafe_mask_maker = SafeMaskMaker(methods=[\"aeff-max\"], aeff_percent=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets = Datasets()\n\nfor obs_id, observation in zip(obs_ids, observations):\n    dataset = dataset_maker.run(dataset_empty.copy(name=str(obs_id)), observation)\n    dataset_on_off = bkg_maker.run(dataset, observation)\n    dataset_on_off = safe_mask_maker.run(dataset_on_off, observation)\n    datasets.append(dataset_on_off)\n\nprint(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot off regions\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nax = exclusion_mask.plot()\non_region.to_pixel(ax.wcs).plot(ax=ax, edgecolor=\"k\")\nplot_spectrum_datasets_off_regions(ax=ax, datasets=datasets)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Source statistic\n\nNext we\u2019re going to look at the overall source statistics in our signal\nregion.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "info_table = datasets.info_table(cumulative=True)\n\ndisplay(info_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And make the corresponding plots\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax_excess, ax_sqrt_ts) = plt.subplots(figsize=(10, 4), ncols=2, nrows=1)\nax_excess.plot(\n    info_table[\"livetime\"].to(\"h\"),\n    info_table[\"excess\"],\n    marker=\"o\",\n    ls=\"none\",\n)\n\nax_excess.set_title(\"Excess\")\nax_excess.set_xlabel(\"Livetime [h]\")\nax_excess.set_ylabel(\"Excess events\")\n\nax_sqrt_ts.plot(\n    info_table[\"livetime\"].to(\"h\"),\n    info_table[\"sqrt_ts\"],\n    marker=\"o\",\n    ls=\"none\",\n)\n\nax_sqrt_ts.set_title(\"Sqrt(TS)\")\nax_sqrt_ts.set_xlabel(\"Livetime [h]\")\nax_sqrt_ts.set_ylabel(\"Sqrt(TS)\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally you can write the extracted datasets to disk using the OGIP\nformat (PHA, ARF, RMF, BKG, see\n[here](https://gamma-astro-data-formats.readthedocs.io/en/latest/spectra/ogip/index.html)_\nfor details):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "path = Path(\"spectrum_analysis\")\npath.mkdir(exist_ok=True)\n\nfor dataset in datasets:\n    dataset.write(filename=path / f\"obs_{dataset.name}.fits.gz\", overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you want to read back the datasets from disk you can use:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets = Datasets()\n\nfor obs_id in obs_ids:\n    filename = path / f\"obs_{obs_id}.fits.gz\"\n    datasets.append(SpectrumDatasetOnOff.read(filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit spectrum\n\nNow we\u2019ll fit a global model to the spectrum. First we do a joint\nlikelihood fit to all observations. If you want to stack the\nobservations see below. We will also produce a debug plot in order to\nshow how the global fit matches one of the individual observations.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spectral_model = ExpCutoffPowerLawSpectralModel(\n    amplitude=1e-12 * u.Unit(\"cm-2 s-1 TeV-1\"),\n    index=2,\n    lambda_=0.1 * u.Unit(\"TeV-1\"),\n    reference=1 * u.TeV,\n)\nmodel = SkyModel(spectral_model=spectral_model, name=\"crab\")\n\ndatasets.models = [model]\n\nfit_joint = Fit()\nresult_joint = fit_joint.run(datasets=datasets)\n\n# we make a copy here to compare it later\nmodel_best_joint = model.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit quality and model residuals\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the results dictionary to see if the fit converged:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_joint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and check the best-fit parameters\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(result_joint.models.to_parameters_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple way to inspect the model residuals is using the function\n`~SpectrumDataset.plot_fit()`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax_spectrum, ax_residuals = datasets[0].plot_fit()\nax_spectrum.set_ylim(0.1, 40)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For more ways of assessing fit quality, please refer to the dedicated\n:doc:`/tutorials/api/fitting` tutorial.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Flux Points\n\nTo round up our analysis we can compute flux points by fitting the norm\nof the global model in energy bands.\nWe create an instance of the\n`~gammapy.estimators.FluxPointsEstimator`, by passing the dataset and\nthe energy binning:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpe = FluxPointsEstimator(\n    energy_edges=energy_axis.edges, source=\"crab\", selection_optional=\"all\"\n)\nflux_points = fpe.run(datasets=datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is a the table of the resulting flux points:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(flux_points.to_table(sed_type=\"dnde\", formatted=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we plot the flux points and their likelihood profiles. For the\nplotting of upper limits we choose a threshold of TS < 4.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nflux_points.plot(ax=ax, sed_type=\"e2dnde\", color=\"darkorange\")\nflux_points.plot_ts_profiles(ax=ax, sed_type=\"e2dnde\")\nax.set_xlim(0.6, 40)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note: it is also possible to plot the flux distribution with the spectral model overlaid,\nbut you must ensure the axis binning is identical for the flux points and\nintegral flux.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The final plot with the best fit model, flux points and residuals can be\nquickly made like this:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flux_points_dataset = FluxPointsDataset(\n    data=flux_points, models=model_best_joint.copy()\n)\nax, _ = flux_points_dataset.plot_fit()\nax.set_xlim(0.6, 40)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stack observations\n\nAn alternative approach to fitting the spectrum is stacking all\nobservations first and the fitting a model. For this we first stack the\nindividual datasets:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_stacked = Datasets(datasets).stack_reduce()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Again we set the model on the dataset we would like to fit (in this case\nit\u2019s only a single one) and pass it to the `~gammapy.modeling.Fit`\nobject:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_stacked.models = model\nstacked_fit = Fit()\nresult_stacked = stacked_fit.run([dataset_stacked])\n\n# Make a copy to compare later\nmodel_best_stacked = model.copy()\n\nprint(result_stacked)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And display the parameter table\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(model_best_joint.parameters.to_table())\n\ndisplay(model_best_stacked.parameters.to_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we compare the results of our stacked analysis to a previously\npublished Crab Nebula Spectrum for reference. This is available in\n`~gammapy.modeling.models.create_crab_spectral_model`.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n\nplot_kwargs = {\n    \"energy_bounds\": [0.1, 30] * u.TeV,\n    \"sed_type\": \"e2dnde\",\n    \"yunits\": u.Unit(\"erg cm-2 s-1\"),\n    \"ax\": ax,\n}\n\n# plot stacked model\nmodel_best_stacked.spectral_model.plot(**plot_kwargs, label=\"Stacked analysis result\")\nmodel_best_stacked.spectral_model.plot_error(facecolor=\"blue\", alpha=0.3, **plot_kwargs)\n\n# plot joint model\nmodel_best_joint.spectral_model.plot(\n    **plot_kwargs, label=\"Joint analysis result\", ls=\"--\"\n)\nmodel_best_joint.spectral_model.plot_error(facecolor=\"orange\", alpha=0.3, **plot_kwargs)\n\ncreate_crab_spectral_model(\"hess_ecpl\").plot(\n    **plot_kwargs,\n    label=\"Crab reference\",\n)\nax.legend()\nplt.show()\n\n# sphinx_gallery_thumbnail_number = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A note on statistics\n\nDifferent statistic are available for the `~gammapy.datasets.FluxPointsDataset` :\n\n- chi2 : estimate from chi2 statistics.\n- profile : estimate from interpolation of the likelihood profile.\n- distrib : estimate from probability distributions,\n            assuming that flux points correspond to asymmetric gaussians\n            and upper limits complementary error functions.\n\nDefault is `chi2`, in that case upper limits are ignored and the mean of asymetrics error is used.\nSo it is recommended to use `profile` if `stat_scan` is available on flux points.\nThe `distrib` case provides an approximation if the `profile` is not available\nwhich allows to take into accounts upper limit and asymetrics error.\n\nIn the example below we can see that the `profile` case matches exactly the result\nfrom the joint analysis of the ON/OFF datasets using `wstat` (as labelled).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_stat(fp_dataset):\n    fig, ax = plt.subplots()\n\n    plot_kwargs = {\n        \"energy_bounds\": [0.1, 30] * u.TeV,\n        \"sed_type\": \"e2dnde\",\n        \"ax\": ax,\n    }\n\n    fp_dataset.data.plot(energy_power=2, ax=ax)\n    model_best_joint.spectral_model.plot(\n        color=\"b\", lw=0.5, **plot_kwargs, label=\"wstat\"\n    )\n\n    stat_types = [\"chi2\", \"profile\", \"distrib\"]\n    colors = [\"red\", \"g\", \"c\"]\n    lss = [\"--\", \":\", \"--\"]\n\n    for ks, stat in enumerate(stat_types):\n        fp_dataset.stat_type = stat\n\n        fit = Fit()\n        fit.run([fp_dataset])\n\n        fp_dataset.models[0].spectral_model.plot(\n            color=colors[ks], ls=lss[ks], **plot_kwargs, label=stat\n        )\n        fp_dataset.models[0].spectral_model.plot_error(\n            facecolor=colors[ks], **plot_kwargs\n        )\n        plt.legend()\n\n\nplot_stat(flux_points_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to avoid discrepancies due to the treatment of upper limits\nwe can utilise the `~gammapy.estimators.utils.resample_energy_edges`\nfor defining energy bins in which the minimum number of `sqrt_ts` is 2.\nIn that case all the statistics definitions give equivalent results.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "energy_edges = resample_energy_edges(dataset_stacked, conditions={\"sqrt_ts_min\": 2})\n\nfpe_no_ul = FluxPointsEstimator(\n    energy_edges=energy_edges, source=\"crab\", selection_optional=\"all\"\n)\nflux_points_no_ul = fpe_no_ul.run(datasets=datasets)\nflux_points_dataset_no_ul = FluxPointsDataset(\n    data=flux_points_no_ul,\n    models=model_best_joint.copy(),\n)\n\nplot_stat(flux_points_dataset_no_ul)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercises\n\nNow you have learned the basics of a spectral analysis with Gammapy. To\npractice you can continue with the following exercises:\n\n-  Fit a different spectral model to the data. You could try\n   `~gammapy.modeling.models.ExpCutoffPowerLawSpectralModel` or\n   `~gammapy.modeling.models.LogParabolaSpectralModel`.\n-  Compute flux points for the stacked dataset.\n-  Create a `~gammapy.datasets.FluxPointsDataset` with the flux points\n   you have computed for the stacked dataset and fit the flux points\n   again with obe of the spectral models. How does the result compare to\n   the best fit model, that was directly fitted to the counts data?\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What next?\n\nThe methods shown in this tutorial is valid for point-like or midly\nextended sources where we can assume that the IRF taken at the region\ncenter is valid over the whole region. If one wants to extract the 1D\nspectrum of a large source and properly average the response over the\nextraction region, one has to use a different approach explained in\nthe :doc:`/tutorials/analysis-1d/extended_source_spectral_analysis`\ntutorial.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}