{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Fitting\n\nLearn how the model, dataset and fit Gammapy classes work together in a detailed modeling and fitting use-case.\n\n## Prerequisites\n\n-  Knowledge of spectral analysis to produce 1D On-Off datasets, see\n   the :doc:`/tutorials/analysis-1d/spectral_analysis` tutorial.\n-  Reading of pre-computed datasets see e.g.\n   :doc:`/tutorials/analysis-3d/analysis_mwl` tutorial.\n-  General knowledge on statistics and optimization methods\n\n## Proposed approach\n\nThis is a hands-on tutorial to `~gammapy.modeling`, showing how to do\nperform a Fit in gammapy. The emphasis here is on interfacing the\n`Fit` class and inspecting the errors. To see an analysis example of\nhow datasets and models interact, see the :doc:`/tutorials/api/model_management` tutorial.\nAs an example, in this notebook, we are going to work with HESS data of the Crab Nebula and show in\nparticular how to :\n\n- perform a spectral analysis\n- use different fitting backends\n- access covariance matrix information and parameter errors\n- compute likelihood profile - compute confidence contours\n\nSee also: :doc:`/tutorials/api/models` and `modeling`.\n\n## The setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\nimport numpy as np\nfrom astropy import units as u\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom gammapy.datasets import Datasets, SpectrumDatasetOnOff\nfrom gammapy.modeling import Fit\nfrom gammapy.modeling.models import LogParabolaSpectralModel, SkyModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gammapy.utils.check import check_tutorials_setup\nfrom gammapy.visualization.utils import plot_contour_line\n\ncheck_tutorials_setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model and dataset\n\nFirst we define the source model, here we need only a spectral model for\nwhich we choose a log-parabola\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "crab_spectrum = LogParabolaSpectralModel(\n    amplitude=1e-11 / u.cm**2 / u.s / u.TeV,\n    reference=1 * u.TeV,\n    alpha=2.3,\n    beta=0.2,\n)\n\ncrab_spectrum.alpha.max = 3\ncrab_spectrum.alpha.min = 1\ncrab_model = SkyModel(spectral_model=crab_spectrum, name=\"crab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data and background are read from pre-computed ON/OFF datasets of\nHESS observations, for simplicity we stack them together. Then we set\nthe model and fit range to the resulting dataset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets = []\nfor obs_id in [23523, 23526]:\n    dataset = SpectrumDatasetOnOff.read(\n        f\"$GAMMAPY_DATA/joint-crab/spectra/hess/pha_obs{obs_id}.fits\"\n    )\n    datasets.append(dataset)\n\ndataset_hess = Datasets(datasets).stack_reduce(name=\"HESS\")\ndatasets = Datasets(datasets=[dataset_hess])\n\n# Set model and fit range\ndataset_hess.models = crab_model\ne_min = 0.66 * u.TeV\ne_max = 30 * u.TeV\ndataset_hess.mask_fit = dataset_hess.counts.geom.energy_mask(e_min, e_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting options\n\nFirst let\u2019s create a `Fit` instance:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scipy_opts = {\n    \"method\": \"L-BFGS-B\",\n    \"options\": {\"ftol\": 1e-4, \"gtol\": 1e-05},\n    \"backend\": \"scipy\",\n}\nfit_scipy = Fit(store_trace=True, optimize_opts=scipy_opts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By default the fit is performed using MINUIT, you can select alternative\noptimizers and set their option using the `optimize_opts` argument of\nthe `Fit.run()` method. In addition we have specified to store the\ntrace of parameter values of the fit.\n\nNote that, for now, covaraince matrix and errors are computed only for\nthe fitting with MINUIT. However depending on the problem other\noptimizers can better perform, so sometimes it can be useful to run a\npre-fit with alternative optimization methods.\n\n| For the \u201cscipy\u201d backend the available options are described in detail\n  here:\n| https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_scipy = fit_scipy.run(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| For the \u201csherpa\u201d backend you can choose the optimization algorithm\n  between method = {\u201csimplex\u201d, \u201clevmar\u201d, \u201cmoncar\u201d, \u201cgridsearch\u201d}.\n| Those methods are described and compared in detail on\n  http://cxc.cfa.harvard.edu/sherpa/methods/index.html The available\n  options of the optimization methods are described on the following\n  page https://cxc.cfa.harvard.edu/sherpa/methods/opt_methods.html\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sherpa_opts = {\"method\": \"simplex\", \"ftol\": 1e-3, \"maxfev\": int(1e4)}\nfit_sherpa = Fit(store_trace=True, backend=\"sherpa\", optimize_opts=sherpa_opts)\nresults_simplex = fit_sherpa.run(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the \u201cminuit\u201d backend see\nhttps://iminuit.readthedocs.io/en/latest/reference.html for a detailed\ndescription of the available options. If there is an entry\n\u2018migrad_opts\u2019, those options will be passed to\n[iminuit.Minuit.migrad](https://iminuit.readthedocs.io/en/latest/reference.html#iminuit.Minuit.migrad)_.\nAdditionally you can set the fit tolerance using the\n[tol](https://iminuit.readthedocs.io/en/latest/reference.html#iminuit.Minuit.tol)_\noption. The minimization will stop when the estimated distance to the\nminimum is less than 0.001*tol (by default tol=0.1). The\n[strategy](https://iminuit.readthedocs.io/en/latest/reference.html#iminuit.Minuit.strategy)_\noption change the speed and accuracy of the optimizer: 0 fast, 1\ndefault, 2 slow but accurate. If you want more reliable error estimates,\nyou should run the final fit with strategy 2.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit = Fit(store_trace=True)\nminuit_opts = {\"tol\": 0.001, \"strategy\": 1}\nfit.backend = \"minuit\"\nfit.optimize_opts = minuit_opts\nresult_minuit = fit.run(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit quality assessment\n\nThere are various ways to check the convergence and quality of a fit.\nAmong them:\n\nRefer to the automatically-generated results dictionary:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_scipy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(results_simplex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_minuit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the fit is performed with minuit you can print detailed informations\nto check the convergence\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fit.minuit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the trace of the fit e.g.\u00a0 in case the fit did not converge\nproperly\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(result_minuit.trace)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fitted models are copied on the `~gammapy.modeling.FitResult` object.\nThey can be inspected to check that the fitted values and errors\nfor all parameters are reasonable, and no fitted parameter value is \u201ctoo close\u201d\n- or even outside - its allowed min-max range\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "display(result_minuit.models.to_parameters_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot fit statistic profiles for all fitted parameters, using\n`~gammapy.modeling.Fit.stat_profile`. For a good fit and error\nestimate each profile should be parabolic. The specification for each\nfit statistic profile can be changed on the\n`~gammapy.modeling.Parameter` object, which has `~gammapy.modeling.Parameter.scan_min`,\n`~gammapy.modeling.Parameter.scan_max`, `~gammapy.modeling.Parameter.scan_n_values` and `~gammapy.modeling.Parameter.scan_n_sigma` attributes.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_stat = result_minuit.total_stat\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14, 4))\n\nfor ax, par in zip(axes, datasets.parameters.free_parameters):\n    par.scan_n_values = 17\n    idx = datasets.parameters.index(par)\n    name = datasets.models.parameters_unique_names[idx]\n    profile = fit.stat_profile(datasets=datasets, parameter=par)\n    ax.plot(profile[f\"{name}_scan\"], profile[\"stat_scan\"] - total_stat)\n    ax.set_xlabel(f\"{par.name} [{par.unit}]\")\n    ax.set_ylabel(\"Delta TS\")\n    ax.set_title(f\"{name}:\\n {par.value:.1e} +- {par.error:.1e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect model residuals. Those can always be accessed using\n`~gammapy.datasets.Dataset.residuals()`. For more details, we refer here to the dedicated\n:doc:`/tutorials/analysis-3d/analysis_3d` (for `~gammapy.datasets.MapDataset` fitting) and\n:doc:`/tutorials/analysis-1d/spectral_analysis` (for `SpectrumDataset` fitting).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Covariance and parameters errors\n\nAfter the fit the covariance matrix is attached to the models copy\nstored on the `~gammapy.modeling.FitResult` object.\nYou can access it directly with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_minuit.models.covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And you can plot the total parameter correlation as well:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_minuit.models.covariance.plot_correlation()\n\n# The covariance information is also propagated to the individual models\n# Therefore, one can also get the error on a specific parameter by directly\n# accessing the `~gammapy.modeling.Parameter.error` attribute:\n#\n\nprint(crab_model.spectral_model.alpha.error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As an example, this step is needed to produce a butterfly plot showing\nthe envelope of the model taking into account parameter uncertainties.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure()\nenergy_bounds = [1, 10] * u.TeV\ncrab_spectrum.plot(energy_bounds=energy_bounds, energy_power=2)\nax = crab_spectrum.plot_error(energy_bounds=energy_bounds, energy_power=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confidence contours\n\nIn most studies, one wishes to estimate parameters distribution using\nobserved sample data. A 1-dimensional confidence interval gives an\nestimated range of values which is likely to include an unknown\nparameter. A confidence contour is a 2-dimensional generalization of a\nconfidence interval, often represented as an ellipsoid around the\nbest-fit value.\n\nGammapy offers two ways of computing confidence contours, in the\ndedicated methods `~gammapy.modeling.Fit.minos_contour` and `~gammapy.modeling.Fit.stat_profile`. In\nthe following sections we will describe them.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An important point to keep in mind is: *what does a $N\\sigma$\nconfidence contour really mean?* The answer is it represents the points\nof the parameter space for which the model likelihood is $N\\sigma$\nabove the minimum. But one always has to keep in mind that **1 standard\ndeviation in two dimensions has a smaller coverage probability than\n68%**, and similarly for all other levels. In particular, in\n2-dimensions the probability enclosed by the $N\\sigma$ confidence\ncontour is $P(N)=1-e^{-N^2/2}$.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing contours using `~gammapy.modeling.Fit.stat_contour`\n\nAfter the fit, MINUIT offers the possibility to compute the confidence\nconfours. gammapy provides an interface to this functionality through\nthe `~gammapy.modeling.Fit` object using the `~gammapy.modeling.Fit.stat_contour` method. Here we defined a\nfunction to automate the contour production for the different\nparameter and confidence levels (expressed in terms of sigma):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def make_contours(fit, datasets, result, npoints, sigmas):\n    cts_sigma = []\n    for sigma in sigmas:\n        contours = dict()\n        for par_1, par_2 in combinations([\"alpha\", \"beta\", \"amplitude\"], r=2):\n            idx1, idx2 = datasets.parameters.index(par_1), datasets.parameters.index(\n                par_2\n            )\n            name1 = datasets.models.parameters_unique_names[idx1]\n            name2 = datasets.models.parameters_unique_names[idx2]\n            contour = fit.stat_contour(\n                datasets=datasets,\n                x=datasets.parameters[par_1],\n                y=datasets.parameters[par_2],\n                numpoints=npoints,\n                sigma=sigma,\n            )\n            contours[f\"contour_{par_1}_{par_2}\"] = {\n                par_1: contour[name1].tolist(),\n                par_2: contour[name2].tolist(),\n            }\n        cts_sigma.append(contours)\n    return cts_sigma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compute few contours.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigmas = [1, 2]\ncts_sigma = make_contours(\n    fit=fit,\n    datasets=datasets,\n    result=result_minuit,\n    npoints=10,\n    sigmas=sigmas,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we prepare some aliases and annotations in order to make the\nplotting nicer.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pars = {\n    \"phi\": r\"$\\phi_0 \\,/\\,(10^{-11}\\,{\\rm TeV}^{-1} \\, {\\rm cm}^{-2} {\\rm s}^{-1})$\",\n    \"alpha\": r\"$\\alpha$\",\n    \"beta\": r\"$\\beta$\",\n}\n\npanels = [\n    {\n        \"x\": \"alpha\",\n        \"y\": \"phi\",\n        \"cx\": (lambda ct: ct[\"contour_alpha_amplitude\"][\"alpha\"]),\n        \"cy\": (lambda ct: np.array(1e11) * ct[\"contour_alpha_amplitude\"][\"amplitude\"]),\n    },\n    {\n        \"x\": \"beta\",\n        \"y\": \"phi\",\n        \"cx\": (lambda ct: ct[\"contour_beta_amplitude\"][\"beta\"]),\n        \"cy\": (lambda ct: np.array(1e11) * ct[\"contour_beta_amplitude\"][\"amplitude\"]),\n    },\n    {\n        \"x\": \"alpha\",\n        \"y\": \"beta\",\n        \"cx\": (lambda ct: ct[\"contour_alpha_beta\"][\"alpha\"]),\n        \"cy\": (lambda ct: ct[\"contour_alpha_beta\"][\"beta\"]),\n    },\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we produce the confidence contours figures.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\ncolors = [\"m\", \"b\", \"c\"]\nfor p, ax in zip(panels, axes):\n    xlabel = pars[p[\"x\"]]\n    ylabel = pars[p[\"y\"]]\n    for ks in range(len(cts_sigma)):\n        plot_contour_line(\n            ax,\n            p[\"cx\"](cts_sigma[ks]),\n            p[\"cy\"](cts_sigma[ks]),\n            lw=2.5,\n            color=colors[ks],\n            label=f\"{sigmas[ks]}\" + r\"$\\sigma$\",\n        )\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\nplt.legend()\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Computing contours using `~gammapy.modeling.Fit.stat_surface`\n\nThis alternative method for the computation of confidence contours,\nalthough more time consuming than `~gammapy.modeling.Fit.minos_contour()`, is expected\nto be more stable. It consists of a generalization of\n`~gammapy.modeling.Fit.stat_profile()` to a 2-dimensional parameter space. The algorithm\nis very simple: - First, passing two arrays of parameters values, a\n2-dimensional discrete parameter space is defined; - For each node of\nthe parameter space, the two parameters of interest are frozen. This\nway, a likelihood value ($-2\\mathrm{ln}\\,\\mathcal{L}$, actually)\nis computed, by either freezing (default) or fitting all nuisance\nparameters; - Finally, a 2-dimensional surface of\n$-2\\mathrm{ln}(\\mathcal{L})$ values is returned. Using that\nsurface, one can easily compute a surface of\n$TS = -2\\Delta\\mathrm{ln}(\\mathcal{L})$ and compute confidence\ncontours.\n\nLet\u2019s see it step by step.\n\nFirst of all, we can notice that this method is \u201cbackend-agnostic\u201d,\nmeaning that it can be run with MINUIT, sherpa or scipy as fitting\ntools. Here we will stick with MINUIT, which is the default choice:\n\nAs an example, we can compute the confidence contour for the `alpha`\nand `beta` parameters of the `dataset_hess`. Here we define the\nparameter space:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = result_minuit\npar_alpha = datasets.parameters[\"alpha\"]\npar_beta = datasets.parameters[\"beta\"]\n\npar_alpha.scan_values = np.linspace(1.55, 2.7, 20)\npar_beta.scan_values = np.linspace(-0.05, 0.55, 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we run the algorithm, by choosing `reoptimize=False` for the sake\nof time saving. In real life applications, we strongly recommend to use\n`reoptimize=True`, so that all free nuisance parameters will be fit at\neach grid node. This is the correct way, statistically speaking, of\ncomputing confidence contours, but is expected to be time consuming.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fit = Fit(backend=\"minuit\", optimize_opts={\"print_level\": 0})\nstat_surface = fit.stat_surface(\n    datasets=datasets,\n    x=par_alpha,\n    y=par_beta,\n    reoptimize=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to easily inspect the results, we can convert the\n$-2\\mathrm{ln}(\\mathcal{L})$ surface to a surface of statistical\nsignificance (in units of Gaussian standard deviations from the surface\nminimum):\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute TS\nTS = stat_surface[\"stat_scan\"] - result.total_stat\n\n# Compute the corresponding statistical significance surface\nstat_surface = np.sqrt(TS.T)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that, as explained before, $1\\sigma$ contour obtained this\nway will not contain 68% of the probability, but rather\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute the corresponding statistical significance surface\n# p_value = 1 - st.chi2(df=1).cdf(TS)\n# gaussian_sigmas = st.norm.isf(p_value / 2).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can plot the surface values together with contours:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\nx_values = par_alpha.scan_values\ny_values = par_beta.scan_values\n\n# plot surface\nim = ax.pcolormesh(x_values, y_values, stat_surface, shading=\"auto\")\nfig.colorbar(im, label=\"sqrt(TS)\")\nax.set_xlabel(f\"{par_alpha.name}\")\nax.set_ylabel(f\"{par_beta.name}\")\n\n# We choose to plot 1 and 2 sigma confidence contours\nlevels = [1, 2]\ncontours = ax.contour(x_values, y_values, stat_surface, levels=levels, colors=\"white\")\nax.clabel(contours, fmt=\"%.0f $\\\\sigma$\", inline=3, fontsize=15)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that, if computed with `reoptimize=True`, this plot would be\ncompletely consistent with the third panel of the plot produced with\n`~gammapy.modeling.Fit.stat_contour` (try!).\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, it is always remember that confidence contours are\napproximations. In particular, when the parameter range boundaries are\nclose to the contours lines, it is expected that the statistical meaning\nof the contours is not well defined. That\u2019s why we advise to always\nchoose a parameter space that contains the contours you\u2019re interested\nin.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}