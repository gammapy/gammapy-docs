{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimation of time variability in a lightcurve\nCompute a series of time variability significance estimators for a lightcurve.\n\n## Prerequisites\n\nUnderstanding the light curve estimator, please refer to the :doc:`light curve notebook </tutorials/analysis-time/light_curve`.\nFor more in-depth explanation on the creation of smaller observations for exploring time variability, refer to :doc:`light curve notebook </tutorials/analysis-time/light_curve_flare`\n\n## Context\nFrequently, after computing a lightcurve, we need to quantify its variability in the time domain, for example in the case of a flare, burst, decaying light curve in GRBs or heightened activity in general.\n\nThere are many ways to define the significance of the variability.\n\n**Objective: Estimate the level time variability in a lightcurve through different methods.**\n\n## Proposed approach\nWe will start by reading the pre-computed light curve for PKS 2155-304 that is stored in `$GAMMAPY_DATA` \nTo learn how to compute such an object, see the :doc:`light curve tutorial </tutorials/analysis-time/light_curve_flare`\n\nThis tutorial will demonstrate how to compute different estimates which measure the significance of variability. These estimators range from basic ones that calculate the peak-to-trough variation, to more complex ones like fractional excess and point-to-point fractional variance, which consider the entire light curve. We also show an approach which utilises the change points inn Bayesian blocks as indicators of variability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\nAs usual, we\u2019ll start with some general imports\u2026\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom astropy.stats import bayesian_blocks\nimport matplotlib.pyplot as plt\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.estimators.utils import (\n    compute_lightcurve_doublingtime,\n    compute_lightcurve_fpp,\n    compute_lightcurve_fvar,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the light curve for the PKS 2155-304 flare directly from \u201c$GAMMAPY_DATA/estimators\u201d\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_1d = FluxPoints.read(\n    \"$GAMMAPY_DATA/estimators/pks2155_hess_lc/pks2155_hess_lc.fits\", format=\"lightcurve\"\n)\n\nplt.figure(figsize=(8, 6))\nlc_1d.plot(marker=\"o\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods to characterize variability\n Amplitude maximum variation, relative variability amplitude and variability amplitude\n\nThe amplitude maximum variation is the simplest method to define variability (as described in\n[Boller et al., 2016](https://ui.adsabs.harvard.edu/abs/2016A&A...588A.103B/abstract)) as it just computes\nthe level of tension between the lowest and highest measured fluxes in the lightcurve.\nThis estimator requires fully Gaussian errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flux = lc_1d.flux.quantity\nflux_err = lc_1d.flux_err.quantity\n\nf_mean = np.mean(flux)\nf_mean_err = np.mean(flux_err)\n\nf_max = flux.max()\nf_max_err = flux_err[flux.argmax()]\nf_min = flux.min()\nf_min_err = flux_err[flux.argmin()]\n\namplitude_maximum_variation = (f_max - f_max_err) - (f_min + f_min_err)\n\namplitude_maximum_significance = amplitude_maximum_variation / np.sqrt(\n    f_max_err**2 + f_min_err**2\n)\n\nprint(amplitude_maximum_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are other methods based on the peak-to-trough difference to assess the variability in a lightcurve.\nHere we present as example the relative variability amplitude as presented in [Kovalev et al., 2004]\n(https://iopscience.iop.org/article/10.1086/497430):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "relative_variability_amplitude = (f_max - f_min) / (f_max + f_min)\n\nrelative_variability_error = (\n    2\n    * np.sqrt((f_max * f_min_err) ** 2 + (f_min * f_max_err) ** 2)\n    / (f_max + f_min) ** 2\n)\n\nrelative_variability_significance = (\n    relative_variability_amplitude / relative_variability_error\n)\n\nprint(relative_variability_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the variability amplitude as presented in [Heidt & Wagner, 1996](https://ui.adsabs.harvard.edu/abs/1996A%26A...305...42H/abstract):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variability_amplitude = np.sqrt((f_max - f_min) ** 2 - 2 * f_mean_err**2)\n\nvariability_amplitude_100 = 100 * variability_amplitude / f_mean\n\nvariability_amplitude_error = (\n    100\n    * ((f_max - f_min) / (f_mean * variability_amplitude_100 / 100))\n    * np.sqrt(\n        (f_max_err / f_mean) ** 2\n        + (f_min_err / f_mean) ** 2\n        + ((np.std(flux, ddof=1) / np.sqrt(len(flux))) / (f_max - f_mean)) ** 2\n        * (variability_amplitude_100 / 100) ** 4\n    )\n)\n\nvariability_amplitude_significance = (\n    variability_amplitude_100 / variability_amplitude_error\n)\n\nprint(variability_amplitude_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fractional excess variance, point-to-point fractional variance and doubling/halving time\nThe fractional excess variance as presented by\n[Vaughan et al., 2003](https://ui.adsabs.harvard.edu/abs/2003MNRAS.345.1271V/abstract)) is a simple but effective\nmethod to assess the significance of a time variability feature in an object, for example an AGN flare.\nIt is important to note that it requires Gaussian errors to be applicable.\nThe excess variance computation is implemented in `~gammapy.estimators.utils`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fvar_table = compute_lightcurve_fvar(lc_1d)\nprint(fvar_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A similar estimator is the point-to-point fractional variance, as defined by\n[Edelson et al., 2002](https://ui.adsabs.harvard.edu/abs/2002ApJ...568..610E/abstract)\nwhich samples the lightcurve on smaller time granularity.\nIn general, the point-to-point fractional variance being higher than the fractional excess variance is indicative\nof the presence of very short timescale variability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpp_table = compute_lightcurve_fpp(lc_1d)\nprint(fpp_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The characteristic doubling and halving time, as introduced by\n[Brown, 2013](https://durham-repository.worktribe.com/output/1478453/) of the light curve can also be computed.\nThis provides information on the shape of the variability feature, in particular how quickly it rises and falls.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtime_table = compute_lightcurve_doublingtime(lc_1d, flux_quantity=\"flux\")\nprint(dtime_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian blocks\nThe presence of temporal variability in a lightcurve can be assessed by using bayesian blocks ([reference\npaper](https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S/abstract)). A good and simple-to-use implementation of the algorithm is found in `astropy.stats`([documentation](https://docs.astropy.org/en/stable/api/astropy.stats.bayesian_blocks.html)). This implementation uses Gaussian statistics, as opposed to the [first introductory paper](https://iopscience.iop.org/article/10.1086/306064) which was based on Poissonian statistics.\n\nBy passing the flux and error on the flux as `measures` to the method we can obtain the list of optimal bin edges defined by the bayesian blocks algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time = lc_1d.geom.axes[\"time\"].time_mid.mjd\n\nbayesian_edges = bayesian_blocks(\n    t=time, x=flux.flatten(), sigma=flux_err.flatten(), fitness=\"measures\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the difference between the original lightcurve and the rebin with bayesian blocks\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bayesian_flux = []\nfor tmin, tmax in zip(bayesian_edges[:-1], bayesian_edges[1:]):\n    mask = (time >= tmin) & (time <= tmax)\n    bayesian_flux.append(\n        np.average(\n            flux.flatten()[mask], weights=1 / (flux_err.flatten()[mask] ** 2)\n        ).value\n    )\n\nxerr = np.diff(bayesian_edges) / 2\nbayesian_x = bayesian_edges[:-1] + xerr\n\nfig, ax = plt.subplots(\n    figsize=(8, 6),\n    gridspec_kw={\"left\": 0.16, \"bottom\": 0.2, \"top\": 0.98, \"right\": 0.98},\n)\nplt.plot(time, flux.flatten(), marker=\"+\", linestyle=\"\", color=\"teal\")\nplt.errorbar(\n    bayesian_x,\n    bayesian_flux,\n    xerr=np.diff(bayesian_edges) / 2,\n    linestyle=\"\",\n    color=\"orange\",\n)\nplt.ylim(bottom=0)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result giving a significance estimation for variability in the lightcurve is the number of *change points*, i.e. the number of internal bin edges: if at least one change point is identified by the algorithm, there is significant variability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ncp = len(bayesian_edges) - 2\nprint(ncp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}