{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Light curves\n\nCompute per-observation and nightly fluxes of four Crab nebula observations.\n\n## Prerequisites\n\n-  Knowledge of the high level interface to perform data reduction, see\n   [first gammapy analysis with the high level interface\n   tutorial](../../starting/analysis_1.ipynb)_\n\n## Context\n\nThis tutorial presents how light curve extraction is performed in\ngammapy, i.e.\u00a0how to measure the flux of a source in different time\nbins.\n\nCherenkov telescopes usually work with observing runs and distribute\ndata according to this basic time interval. A typical use case is to\nlook for variability of a source on various time binnings: observation\nrun-wise binning, nightly, weekly etc.\n\n**Objective: The Crab nebula is not known to be variable at TeV\nenergies, so we expect constant brightness within statistical and\nsystematic errors. Compute per-observation and nightly fluxes of the\nfour Crab nebula observations from the H.E.S.S. first public test data\nrelease**\\ [o](https://www.mpi-hd.mpg.de/hfm/HESS/pages/dl3-dr1/)_\\ **to\ncheck it.**\n\n## Proposed approach\n\nWe will demonstrate how to compute a light curve from 3D reduced\ndatasets (`~gammapy.datasets.MapDataset`) as well as 1D ON-OFF\nspectral datasets (`~gammapy.datasets.SpectrumDatasetOnOff`).\n\nThe data reduction will be performed with the high level interface for\nthe data reduction. Then we will use the\n`~gammapy.estimators.LightCurveEstimator` class, which is able to\nextract a light curve independently of the dataset type.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n\nAs usual, we\u2019ll start with some general imports\u2026\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport astropy.units as u\nfrom astropy.coordinates import SkyCoord\nfrom astropy.time import Time\n\n# %matplotlib inline\nimport matplotlib.pyplot as plt\nfrom gammapy.analysis import Analysis, AnalysisConfig\nfrom gammapy.estimators import LightCurveEstimator\nfrom gammapy.modeling.models import (\n    Models,\n    PointSpatialModel,\n    PowerLawSpectralModel,\n    SkyModel,\n)\n\nlog = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check setup\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from gammapy.utils.check import check_tutorials_setup\n\ncheck_tutorials_setup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis configuration\n\nFor the 1D and 3D extraction, we will use the same CrabNebula\nconfiguration than in the notebook analysis_1.ipynb using the high level\ninterface of Gammapy.\n\nFrom the high level interface, the data reduction for those observations\nis performed as followed\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the 3D analysis configuration\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conf_3d = AnalysisConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Definition of the data selection\n\nHere we use the Crab runs from the HESS DL3 data release 1\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conf_3d.observations.obs_ids = [23523, 23526, 23559, 23592]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Definition of the dataset geometry\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We want a 3D analysis\nconf_3d.datasets.type = \"3d\"\n\n# We want to extract the data by observation and therefore to not stack them\nconf_3d.datasets.stack = False\n\n# Here is the WCS geometry of the Maps\nconf_3d.datasets.geom.wcs.skydir = dict(\n    frame=\"icrs\", lon=83.63308 * u.deg, lat=22.01450 * u.deg\n)\nconf_3d.datasets.geom.wcs.binsize = 0.02 * u.deg\nconf_3d.datasets.geom.wcs.width = dict(width=1 * u.deg, height=1 * u.deg)\n\n# We define a value for the IRF Maps binsize\nconf_3d.datasets.geom.wcs.binsize_irf = 0.2 * u.deg\n\n# Define energy binning for the Maps\nconf_3d.datasets.geom.axes.energy = dict(min=0.7 * u.TeV, max=10 * u.TeV, nbins=5)\nconf_3d.datasets.geom.axes.energy_true = dict(min=0.3 * u.TeV, max=20 * u.TeV, nbins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the 3D data reduction\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analysis_3d = Analysis(conf_3d)\nanalysis_3d.get_observations()\nanalysis_3d.get_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the model to be used\n\nHere we don\u2019t try to fit the model parameters to the whole dataset, but\nwe use predefined values instead.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_position = SkyCoord(ra=83.63308, dec=22.01450, unit=\"deg\")\nspatial_model = PointSpatialModel(\n    lon_0=target_position.ra, lat_0=target_position.dec, frame=\"icrs\"\n)\n\nspectral_model = PowerLawSpectralModel(\n    index=2.702,\n    amplitude=4.712e-11 * u.Unit(\"1 / (cm2 s TeV)\"),\n    reference=1 * u.TeV,\n)\n\nsky_model = SkyModel(\n    spatial_model=spatial_model, spectral_model=spectral_model, name=\"crab\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We assign them the model to be fitted to each dataset\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = Models([sky_model])\nanalysis_3d.set_models(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Light Curve estimation by observation\n\nWe can now create the light curve estimator.\n\nWe pass it the list of datasets and the name of the model component for\nwhich we want to build the light curve. In a given time bin, the only\nfree parameter of the source is its normalization. We can optionally ask\nfor parameters of other model components to be reoptimized during fit,\nthat is most of the time to fit background normalization in each time\nbin.\n\nIf we don\u2019t set any time interval, the\n`~gammapy.estimators.LightCurveEstimator` is determines the flux of\neach dataset and places it at the corresponding time in the light curve.\nHere one dataset equals to one observing run.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_maker_3d = LightCurveEstimator(\n    energy_edges=[1, 10] * u.TeV, source=\"crab\", reoptimize=False\n)\n# Example showing how to change some parameters from the object itself\nlc_maker_3d.n_sigma_ul = 3  # Number of sigma to use for upper limit computation\nlc_maker_3d.selection_optional = (\n    \"all\"  # Add the computation of upper limits and likelihood profile\n)\nlc_3d = lc_maker_3d.run(analysis_3d.datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The LightCurve object contains a table which we can explore.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Example showing how to change just before plotting the threshold on the signal significance\n# (points vs upper limits), even if this has no effect with this data set.\nlc_3d.sqrt_ts_threshold_ul = 5\nlc_3d.plot(axis_name=\"time\")\n\ntable = lc_3d.to_table(format=\"lightcurve\", sed_type=\"flux\")\ntable[\"time_min\", \"time_max\", \"e_min\", \"e_max\", \"flux\", \"flux_err\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running the light curve extraction in 1D\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building the 1D analysis configuration\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conf_1d = AnalysisConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Definition of the data selection\n\nHere we use the Crab runs from the HESS DL3 data release 1\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conf_1d.observations.obs_ids = [23523, 23526, 23559, 23592]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Definition of the dataset geometry\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We want a 1D analysis\nconf_1d.datasets.type = \"1d\"\n\n# We want to extract the data by observation and therefore to not stack them\nconf_1d.datasets.stack = False\n\n# Here we define the ON region and make sure that PSF leakage is corrected\nconf_1d.datasets.on_region = dict(\n    frame=\"icrs\",\n    lon=83.63308 * u.deg,\n    lat=22.01450 * u.deg,\n    radius=0.1 * u.deg,\n)\nconf_1d.datasets.containment_correction = True\n\n# Finally we define the energy binning for the spectra\nconf_1d.datasets.geom.axes.energy = dict(min=0.7 * u.TeV, max=10 * u.TeV, nbins=5)\nconf_1d.datasets.geom.axes.energy_true = dict(min=0.3 * u.TeV, max=20 * u.TeV, nbins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run the 1D data reduction\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analysis_1d = Analysis(conf_1d)\nanalysis_1d.get_observations()\nanalysis_1d.get_datasets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the model to be used\n\nHere we don\u2019t try to fit the model parameters to the whole dataset, but\nwe use predefined values instead.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_position = SkyCoord(ra=83.63308, dec=22.01450, unit=\"deg\")\n\nspectral_model = PowerLawSpectralModel(\n    index=2.702,\n    amplitude=4.712e-11 * u.Unit(\"1 / (cm2 s TeV)\"),\n    reference=1 * u.TeV,\n)\n\nsky_model = SkyModel(spectral_model=spectral_model, name=\"crab\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We assign the model to be fitted to each dataset. We can use the same\n`~gammapy.modeling.models.SkyModel` as before.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models = Models([sky_model])\nanalysis_1d.set_models(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extracting the light curve\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_maker_1d = LightCurveEstimator(\n    energy_edges=[1, 10] * u.TeV, source=\"crab\", reoptimize=False\n)\nlc_1d = lc_maker_1d.run(analysis_1d.datasets)\n\nlc_1d.geom.axes.names\n\nlc_1d.to_table(sed_type=\"flux\", format=\"lightcurve\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare results\n\nFinally we compare the result for the 1D and 3D lightcurve in a single\nfigure:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = lc_1d.plot(marker=\"o\", label=\"1D\")\nlc_3d.plot(ax=ax, marker=\"o\", label=\"3D\")\nplt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Night-wise LC estimation\n\nHere we want to extract a night curve per night. We define the time\nintervals that cover the three nights.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_intervals = [\n    Time([53343.5, 53344.5], format=\"mjd\", scale=\"utc\"),\n    Time([53345.5, 53346.5], format=\"mjd\", scale=\"utc\"),\n    Time([53347.5, 53348.5], format=\"mjd\", scale=\"utc\"),\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To compute the LC on the time intervals defined above, we pass the\n`LightCurveEstimator` the list of time intervals.\n\nInternally, datasets are grouped per time interval and a flux extraction\nis performed for each group.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_maker_1d = LightCurveEstimator(\n    energy_edges=[1, 10] * u.TeV,\n    time_intervals=time_intervals,\n    source=\"crab\",\n    reoptimize=False,\n    selection_optional=\"all\",\n)\n\nnightwise_lc = lc_maker_1d.run(analysis_1d.datasets)\n\nnightwise_lc.plot(color=\"tab:orange\")\nax = nightwise_lc.plot_ts_profiles()\nax.set_ylim(1e-12, 3e-12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What next?\n\nWhen sources are bright enough to look for variability at small time\nscales, the per-observation time binning is no longer relevant. One can\neasily extend the light curve estimation approach presented above to any\ntime binning. This is demonstrated in the [following\ntutorial](light_curve_flare.ipynb)_ which shows the extraction of the\nlightcurve of an AGN flare.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}