{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Light curves for flares\n\nCompute the light curve of a PKS 2155-304 flare on 10 minutes time intervals.\n\n## Prerequisites\n\n-  Understanding of how the light curve estimator works, please refer to\n   the :doc:`light curve notebook </tutorials/analysis-time/light_curve>`.\n\n## Context\n\nFrequently, especially when studying flares of bright sources, it is\nnecessary to explore the time behaviour of a source on short time\nscales, in particular on time scales shorter than observing runs.\n\nA typical example is given by the flare of PKS 2155-304 during the night\nfrom July 29 to 30 2006. See the [following\narticle](https://ui.adsabs.harvard.edu/abs/2009A%26A...502..749A/abstract)_.\n\n**Objective: Compute the light curve of a PKS 2155-304 flare on 5\nminutes time intervals, i.e. smaller than the duration of individual\nobservations.**\n\n## Proposed approach\n\nWe have seen in the general presentation of the light curve estimator,\nsee the :doc:`light curve notebook </tutorials/analysis-time/light_curve>`, Gammapy produces\ndatasets in a given time interval, by default that of the parent\nobservation. To be able to produce datasets on smaller time steps, it is\nnecessary to split the observations into the required time intervals.\n\nThis is easily performed with the `~gammapy.data.Observations.select_time` method of\n`~gammapy.data.Observations`. If you pass it a list of time intervals\nit will produce a list of time filtered observations in a new\n`~gammapy.data.Observations` object. Data reduction can then be\nperformed and will result in datasets defined on the required time\nintervals and light curve estimation can proceed directly.\n\nIn summary, we have to:\n\n-  Select relevant `~gammapy.data.Observations` from the\n   `~gammapy.data.DataStore`\n-  Apply the time selection in our predefined time intervals to obtain a\n   new `~gammapy.data.Observations`\n-  Perform the data reduction (in 1D or 3D)\n-  Define the source model\n-  Extract the light curve from the reduced dataset\n\nHere, we will use the PKS 2155-304 observations from the\n[H.E.S.S. first public test data release](https://hess-experiment.eu/releases/)_.\nWe will use time intervals of 5 minutes\nduration. The tutorial is implemented with the intermediate level API.\n\n## Setup\n\nAs usual, we\u2019ll start with some general imports\u2026\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport numpy as np\nimport astropy.units as u\nfrom astropy.coordinates import Angle, SkyCoord\nfrom astropy.time import Time\nfrom regions import CircleSkyRegion\n\n# %matplotlib inline\nimport matplotlib.pyplot as plt\n\nlog = logging.getLogger(__name__)\n\nfrom gammapy.data import DataStore\nfrom gammapy.datasets import Datasets, SpectrumDataset\nfrom gammapy.estimators import LightCurveEstimator\nfrom gammapy.estimators.utils import get_rebinned_axis\nfrom gammapy.makers import (\n    ReflectedRegionsBackgroundMaker,\n    SafeMaskMaker,\n    SpectrumDatasetMaker,\n)\nfrom gammapy.maps import MapAxis, RegionGeom\nfrom gammapy.modeling.models import PowerLawSpectralModel, SkyModel\nfrom gammapy.modeling import Fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Select the data\n\nWe first set the datastore.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_store = DataStore.from_dir(\"$GAMMAPY_DATA/hess-dl3-dr1/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we select observations within 2 degrees of PKS 2155-304.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_position = SkyCoord(329.71693826 * u.deg, -30.2255890 * u.deg, frame=\"icrs\")\nselection = dict(\n    type=\"sky_circle\",\n    frame=\"icrs\",\n    lon=target_position.ra,\n    lat=target_position.dec,\n    radius=2 * u.deg,\n)\nobs_ids = data_store.obs_table.select_observations(selection)[\"OBS_ID\"]\nobservations = data_store.get_observations(obs_ids)\nprint(f\"Number of selected observations : {len(observations)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define time intervals\n\nWe create the list of time intervals, each of duration 10 minutes. Each time interval is an\n`astropy.time.Time` object, containing a start and stop time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t0 = Time(\"2006-07-29T20:30\")\nduration = 10 * u.min\nn_time_bins = 35\ntimes = t0 + np.arange(n_time_bins) * duration\ntime_intervals = [Time([tstart, tstop]) for tstart, tstop in zip(times[:-1], times[1:])]\nprint(time_intervals[0].mjd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter the observations list in time intervals\n\nHere we apply the list of time intervals to the observations with\n`~gammapy.data.Observations.select_time()`.\n\nThis will return a new list of Observations filtered by ``time_intervals``.\nFor each time interval, a new observation is created that converts the\nintersection of the GTIs and time interval.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "short_observations = observations.select_time(time_intervals)\n# check that observations have been filtered\nprint(f\"Number of observations after time filtering: {len(short_observations)}\\n\")\nprint(short_observations[1].gti)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, we have now observations of duration equal to the chosen\ntime step.\n\nNow data reduction and light curve extraction can proceed exactly as\nbefore.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building 1D datasets from the new observations\n\nHere we will perform the data reduction in 1D with reflected regions.\n\n*Beware, with small time intervals the background normalization with OFF\nregions might become problematic.*\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the geometry\n\nWe define the energy axes. As usual, the true energy axis has to cover a\nwider range to ensure a good coverage of the measured energy range\nchosen.\n\nWe need to define the ON extraction region. Its size follows typical\nspectral extraction regions for H.E.S.S. analyses.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Target definition\nenergy_axis = MapAxis.from_energy_bounds(\"0.4 TeV\", \"20 TeV\", nbin=10)\nenergy_axis_true = MapAxis.from_energy_bounds(\n    \"0.1 TeV\", \"40 TeV\", nbin=20, name=\"energy_true\"\n)\n\non_region_radius = Angle(\"0.11 deg\")\non_region = CircleSkyRegion(center=target_position, radius=on_region_radius)\n\ngeom = RegionGeom.create(region=on_region, axes=[energy_axis])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creation of the data reduction makers\n\nWe now create the dataset and background makers for the selected\ngeometry.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_maker = SpectrumDatasetMaker(\n    containment_correction=True, selection=[\"counts\", \"exposure\", \"edisp\"]\n)\nbkg_maker = ReflectedRegionsBackgroundMaker()\nsafe_mask_masker = SafeMaskMaker(methods=[\"aeff-max\"], aeff_percent=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creation of the datasets\n\nNow we perform the actual data reduction in the ``time_intervals``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets = Datasets()\n\ndataset_empty = SpectrumDataset.create(geom=geom, energy_axis_true=energy_axis_true)\n\nfor obs in short_observations:\n    dataset = dataset_maker.run(dataset_empty.copy(), obs)\n\n    dataset_on_off = bkg_maker.run(dataset, obs)\n    dataset_on_off = safe_mask_masker.run(dataset_on_off, obs)\n    datasets.append(dataset_on_off)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define underlying model\n\nSince we use forward folding to obtain the flux points in each bin, exact values will depend on the underlying model. In this example, we use a power law as used in the\n[reference\npaper](https://ui.adsabs.harvard.edu/abs/2009A%26A...502..749A/abstract)_.\n\nAs we have are only using spectral datasets, we do not need any spatial models.\n\n**Note** : All time bins must have the same spectral model. To see how to investigate spectral variability,\nsee :doc:`time resolved spectroscopy notebook </tutorials/analysis-time/time_resolved_spectroscopy>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spectral_model = PowerLawSpectralModel(amplitude=1e-10 * u.Unit(\"1 / (cm2 s TeV)\"))\nsky_model = SkyModel(spatial_model=None, spectral_model=spectral_model, name=\"pks2155\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Assign to model to all datasets\n\nWe assign each dataset its spectral model\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "datasets.models = sky_model\n\n\nfit = Fit()\nresult = fit.run(datasets)\nprint(result.models.to_parameters_table())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract the light curve\n\nWe first create the `~gammapy.estimators.LightCurveEstimator` for the\nlist of datasets we just produced. We give the estimator the name of the\nsource component to be fitted. We can directly compute the light curve in multiple energy\nbins by supplying a list of ``energy_edges``.\n\nBy default, the likelihood scan is computed from 0.2 to 5.0.\nHere, we increase the max value to 10.0, because we are\ndealing with a large flare.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_maker_1d = LightCurveEstimator(\n    energy_edges=[0.7, 1, 20] * u.TeV,\n    source=\"pks2155\",\n    time_intervals=time_intervals,\n    selection_optional=\"all\",\n    n_jobs=4,\n)\nlc_maker_1d.norm.scan_max = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now perform the light curve extraction itself. To compare with\nthe [reference\npaper](https://ui.adsabs.harvard.edu/abs/2009A%26A...502..749A/abstract)_,\nwe select the 0.7-20 TeV range.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_1d = lc_maker_1d.run(datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we plot the result for the 1D lightcurve:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.3)\nlc_1d.plot(marker=\"o\", axis_name=\"time\", sed_type=\"flux\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Light curves once obtained can be rebinned using the likelihood profiles.\nHere, we rebin 3 adjacent bins together, to get 30 minute bins.\n\nWe will first slice ``lc_1d`` to obtain the lightcurve in the first energy bin\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "slices = {\"energy\": slice(0, 1)}\nsliced_lc = lc_1d.slice_by_idx(slices)\nprint(sliced_lc)\n\naxis_new = get_rebinned_axis(\n    sliced_lc, method=\"fixed-bins\", group_size=3, axis_name=\"time\"\n)\nprint(axis_new)\n\nlc_new = sliced_lc.resample_axis(axis_new)\nplt.figure(figsize=(8, 6))\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.3)\nax = sliced_lc.plot(label=\"original\")\nlc_new.plot(ax=ax, label=\"rebinned\")\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the sliced lightcurve to understand the variability,\nas shown in the :doc:`/tutorials/analysis-time/variability_estimation` tutorial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}