{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimation of time variability in a lightcurve\n\nCompute a series of time variability significance estimators for a lightcurve.\n\n## Prerequisites\n\nUnderstanding the light curve estimator, please refer to the :doc:`/tutorials/analysis-time/light_curve` tutorial.\nFor more in-depth explanation on the creation of smaller observations for exploring time variability, refer to the\n:doc:`/tutorials/analysis-time/light_curve_flare` tutorial.\n\n## Context\nFrequently, after computing a lightcurve, we need to quantify its variability in the time domain, for example\nin the case of a flare, burst, decaying light curve in GRBs or heightened activity in general.\n\nThere are many ways to define the significance of the variability.\n\n**Objective: Estimate the level time variability in a lightcurve through different methods.**\n\n## Proposed approach\n\nWe will start by reading the pre-computed light curve for PKS 2155-304 that is stored in `$GAMMAPY_DATA`\nTo learn how to compute such an object, see the :doc:`/tutorials/analysis-time/light_curve_flare` tutorial.\n\nThis tutorial will demonstrate how to compute different estimates which measure the significance of variability.\nThese estimators range from basic ones that calculate the peak-to-trough variation, to more complex ones like\nfractional excess and point-to-point fractional variance, which consider the entire light curve. We also show an\napproach which utilises the change points in Bayesian blocks as indicators of variability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\nAs usual, we\u2019ll start with some general imports\u2026\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom astropy.stats import bayesian_blocks\nfrom astropy.time import Time\nimport matplotlib.pyplot as plt\nfrom gammapy.estimators import FluxPoints\nfrom gammapy.estimators.utils import (\n    compute_lightcurve_doublingtime,\n    compute_lightcurve_fpp,\n    compute_lightcurve_fvar,\n)\nfrom gammapy.maps import TimeMapAxis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the light curve for the PKS 2155-304 flare directly from `$GAMMAPY_DATA/estimators`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lc_1d = FluxPoints.read(\n    \"$GAMMAPY_DATA/estimators/pks2155_hess_lc/pks2155_hess_lc.fits\", format=\"lightcurve\"\n)\n\nplt.figure(figsize=(8, 6))\nplt.subplots_adjust(bottom=0.2, left=0.2)\nlc_1d.plot(marker=\"o\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methods to characterize variability\n\nThe three methods shown here are:\n\n-  amplitude maximum variation\n-  relative variability amplitude\n-  variability amplitude.\n\nThe amplitude maximum variation is the simplest method to define variability (as described in\n[Boller et al., 2016](https://ui.adsabs.harvard.edu/abs/2016A&A...588A.103B/abstract)_)\nas it just computes\nthe level of tension between the lowest and highest measured fluxes in the lightcurve.\nThis estimator requires fully Gaussian errors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "flux = lc_1d.flux.quantity\nflux_err = lc_1d.flux_err.quantity\n\nf_mean = np.mean(flux)\nf_mean_err = np.mean(flux_err)\n\nf_max = flux.max()\nf_max_err = flux_err[flux.argmax()]\nf_min = flux.min()\nf_min_err = flux_err[flux.argmin()]\n\namplitude_maximum_variation = (f_max - f_max_err) - (f_min + f_min_err)\n\namplitude_maximum_significance = amplitude_maximum_variation / np.sqrt(\n    f_max_err**2 + f_min_err**2\n)\n\nprint(amplitude_maximum_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are other methods based on the peak-to-trough difference to assess the variability in a lightcurve.\nHere we present as example the relative variability amplitude as presented in\n[Kovalev et al., 2004](https://iopscience.iop.org/article/10.1086/497430)_:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "relative_variability_amplitude = (f_max - f_min) / (f_max + f_min)\n\nrelative_variability_error = (\n    2\n    * np.sqrt((f_max * f_min_err) ** 2 + (f_min * f_max_err) ** 2)\n    / (f_max + f_min) ** 2\n)\n\nrelative_variability_significance = (\n    relative_variability_amplitude / relative_variability_error\n)\n\nprint(relative_variability_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variability amplitude as presented in\n[Heidt & Wagner, 1996](https://ui.adsabs.harvard.edu/abs/1996A%26A...305...42H/abstract)_ is:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variability_amplitude = np.sqrt((f_max - f_min) ** 2 - 2 * f_mean_err**2)\n\nvariability_amplitude_100 = 100 * variability_amplitude / f_mean\n\nvariability_amplitude_error = (\n    100\n    * ((f_max - f_min) / (f_mean * variability_amplitude_100 / 100))\n    * np.sqrt(\n        (f_max_err / f_mean) ** 2\n        + (f_min_err / f_mean) ** 2\n        + ((np.std(flux, ddof=1) / np.sqrt(len(flux))) / (f_max - f_mean)) ** 2\n        * (variability_amplitude_100 / 100) ** 4\n    )\n)\n\nvariability_amplitude_significance = (\n    variability_amplitude_100 / variability_amplitude_error\n)\n\nprint(variability_amplitude_significance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fractional excess variance, point-to-point fractional variance and doubling/halving time\nThe fractional excess variance, as presented by\n[Vaughan et al., 2003](https://ui.adsabs.harvard.edu/abs/2003MNRAS.345.1271V/abstract)_, is\na simple but effective method to assess the significance of a time variability feature in an object,\nfor example an AGN flare. It is important to note that it requires Gaussian errors to be applicable.\nThe excess variance computation is implemented in `~gammapy.estimators.utils`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fvar_table = compute_lightcurve_fvar(lc_1d)\nprint(fvar_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A similar estimator is the point-to-point fractional variance, as defined by\n[Edelson et al., 2002](https://ui.adsabs.harvard.edu/abs/2002ApJ...568..610E/abstract)_,\nwhich samples the lightcurve on smaller time granularity.\nIn general, the point-to-point fractional variance being higher than the fractional excess variance is indicative\nof the presence of very short timescale variability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fpp_table = compute_lightcurve_fpp(lc_1d)\nprint(fpp_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The characteristic doubling and halving time of the light curve, as introduced by\n[Brown, 2013](https://durham-repository.worktribe.com/output/1478453/)_, can also be computed.\nThis provides information on the shape of the variability feature, in particular how quickly it rises and falls.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtime_table = compute_lightcurve_doublingtime(lc_1d, flux_quantity=\"flux\")\nprint(dtime_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bayesian blocks\nThe presence of temporal variability in a lightcurve can be assessed by using bayesian blocks\n([Scargle et al., 2013](https://ui.adsabs.harvard.edu/abs/2013ApJ...764..167S/abstract)_).\nA good and simple-to-use implementation of the algorithm is found in\n`astropy.stats.bayesian_blocks`.\nThis implementation uses Gaussian statistics, as opposed to the\n[first introductory paper](https://iopscience.iop.org/article/10.1086/306064)_\nwhich is based on Poissonian statistics.\n\nBy passing the flux and error on the flux as ``measures`` to the method we can obtain the list of optimal bin edges\ndefined by the bayesian blocks algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time = lc_1d.geom.axes[\"time\"].time_mid.mjd\n\nbayesian_edges = bayesian_blocks(\n    t=time, x=flux.flatten(), sigma=flux_err.flatten(), fitness=\"measures\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result giving a significance estimation for variability in the lightcurve is the number of *change points*,\ni.e. the number of internal bin edges: if at least one change point is identified by the algorithm,\nthere is significant variability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ncp = len(bayesian_edges) - 2\nprint(ncp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can rebin the lightcurve to compute the one expected with bayesian edges.\nFirst, we adjust the first and last bins of the ``bayesian_edges`` to coincide\nwith the original light curve start and end points.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a new axis:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "axis_original = lc_1d.geom.axes[\"time\"]\nbayesian_edges[0] = axis_original.time_edges[0].value\nbayesian_edges[-1] = axis_original.time_edges[-1].value\nedges = Time(bayesian_edges, format=\"mjd\", scale=axis_original.reference_time.scale)\naxis_new = TimeMapAxis.from_time_edges(edges[:-1], edges[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Rebin the lightcurve:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "resample = lc_1d.resample_axis(axis_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the new lightcurve on top of the old one:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\nplt.subplots_adjust(bottom=0.2, left=0.2)\nax = lc_1d.plot(label=\"original\")\nresample.plot(ax=ax, marker=\"s\", label=\"rebinned\")\nplt.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}